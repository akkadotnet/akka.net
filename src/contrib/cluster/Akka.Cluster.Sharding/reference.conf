###############################################
# Akka Cluster Sharding Reference Config File #
###############################################

# This is the reference config file that contains all the default settings.
# Make your edits/overrides in your application.conf.


# //#sharding-ext-config
# Settings for the ClusterShardingExtension
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities runs on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = off

  # Set this to a time duration to have sharding passivate entities when they have not
  # gotten any message in this long time. Set to 'off' to disable.
  # It is always disabled if `remember-entities` is enabled.
  passivate-idle-entity-after = 120s

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  # Additionally, if an entity doesn't handle the stopMessage
  # after (handoff-timeout - 5.seconds).max(1.second) it will be stopped forcefully
  handoff-timeout = 60s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10s

  # If the shard is remembering entities and can't store state changes
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate. The entity will be restarted after this duration or when
  # the next message for it is received, which ever occurs first.
  entity-restart-backoff = 10s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  journal-plugin-id = ""

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  snapshot-plugin-id = ""

  # Parameter which determines how the coordinator will be store a state
  # valid values either "persistence" or "ddata"
  # The "ddata" mode is experimental, since it depends on the experimental
  # module akka-distributed-data-experimental.
  state-store-mode = "persistence"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  snapshot-after = 1000

  # The shard deletes persistent events (messages and snapshots) after doing snapshot
  # keeping this number of old persistent batches.
  # Batch is of size `snapshot-after`.
  # When set to 0 after snapshot is successfully done all messages with equal or lower sequence number will be deleted.
  # Default value of 2 leaves last maximum 2*`snapshot-after` messages and 3 snapshots (2 old ones + fresh snapshot)
  keep-nr-of-batches = 2

  # Setting for the default shard allocation strategy
  least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    # The difference between number of shards in the region with most shards and
    # the region with least shards must be greater than (>) the `rebalanceThreshold`
    # for the rebalance to occur.
    # It is also the maximum number of shards that will start rebalancing per rebalance-interval
    # 1 gives the best distribution and therefore typically the best choice.
    # Increasing the threshold can result in quicker rebalance but has the
    # drawback of increased difference between number of shards (and therefore load)
    # on different nodes before rebalance will occur.
    rebalance-threshold = 1

    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
  # works only for state-store-mode = "ddata"
  waiting-for-state-timeout = 5s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # works only for state-store-mode = "ddata"
  updating-state-timeout = 5s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role".
  # A lease can be configured in these settings for the coordinator singleton
  coordinator-singleton = "akka.cluster.singleton"

  # The id of the dispatcher to use for ClusterSharding actors.
  # If not specified default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = ""

  # Config path of the lease that each shard must acquire before starting entity actors
  # default is no lease
  # A lease can also be used for the singleton coordinator by settings it in the coordinator-singleton properties
  use-lease = ""

  # The interval between retries for acquiring the lease
  lease-retry-interval = 5s

  # Settings for the Distributed Data replicator.
  # Same layout as akka.cluster.distributed-data.
  # The "role" of the distributed-data configuration is not used. The distributed-data
  # role will be the same as "akka.cluster.sharding.role".
  # Note that there is one Replicator per role and it's not possible
  # to have different distributed-data settings for different sharding entity types.
  # Only used when state-store-mode=ddata
  distributed-data {
    # minCap parameter to MajorityWrite and MajorityRead consistency level.
    majority-min-cap = 5
    #durable.keys = ["shard-*"]

    # When using many entities with "remember entities" the Gossip message
    # can become to large if including to many in same message. Limit to
    # the same number as the number of ORSet per shard.
    max-delta-elements = 5

    # Actor name of the Replicator actor, /system/ddataReplicator
    name = ddataReplicator

    # Replicas are running on members tagged with this role.
    # All members are used if undefined or empty.
    role = ""

    # How often the Replicator should send out gossip information
    gossip-interval = 2 s

    # How often the subscribers will be notified of changes, if any
    notify-subscribers-interval = 500 ms

    # The id of the dispatcher to use for Replicator actors. If not specified
    # default dispatcher is used.
    # If specified you need to define the settings of the actual dispatcher.
    use-dispatcher = ""

    # How often the Replicator checks for pruning of data associated with
    # removed cluster nodes.
    pruning-interval = 30 s

    # How long time it takes (worst case) to spread the data to all other replica nodes.
    # This is used when initiating and completing the pruning process of data associated
    # with removed cluster nodes. The time measurement is stopped when any replica is
    # unreachable, so it should be configured to worst case in a healthy cluster.
    max-pruning-dissemination = 60 s

    # Serialized Write and Read messages are cached when they are sent to
    # several nodes. If no further activity they are removed from the cache
    # after this duration.
    serializer-cache-time-to-live = 10s

    delta-crdt {

        # Some complex deltas grow in size for each update and above this
        # threshold such deltas are discarded and sent as full state instead.
        max-delta-size = 200
    }

    durable {
      # List of keys that are durable. Prefix matching is supported by using * at the
      # end of a key.
      keys = []

      # The markers of that pruning has been performed for a removed node are kept for this
      # time and thereafter removed. If and old data entry that was never pruned is
      # injected and merged with existing data after this time the value will not be correct.
      # This would be possible if replica with durable data didn't participate in the pruning
      # (e.g. it was shutdown) and later started after this time. A durable replica should not
      # be stopped for longer time than this duration and if it is joining again after this
      # duration its data should first be manually removed (from the lmdb directory).
      # It should be in the magnitude of days. Note that there is a corresponding setting
      # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
      pruning-marker-time-to-live = 10 d

      # Fully qualified class name of the durable store actor. It must be a subclass
      # of akka.actor.Actor and handle the protocol defined in
      # akka.cluster.ddata.DurableStore. The class must have a constructor with
      # com.typesafe.config.Config parameter.
      store-actor-class = ""

      use-dispatcher = akka.cluster.distributed-data.durable.pinned-store

      pinned-store {
        executor = thread-pool-executor
        type = PinnedDispatcher
      }
    }

    lmdb {
      # Directory of LMDB file. There are two options:
      # 1. A relative or absolute path to a directory that ends with 'ddata'
      #    the full name of the directory will contain name of the ActorSystem
      #    and its remote port.
      # 2. Otherwise the path is used as is, as a relative or absolute path to
      #    a directory.
      #
      # When running in production you may want to configure this to a specific
      # path (alt 2), since the default directory contains the remote port of the
      # actor system to make the name unique. If using a dynamically assigned
      # port (0) it will be different each time and the previously stored data
      # will not be loaded.
      dir = "ddata"

      # Size in bytes of the memory mapped file.
      map-size = 104857600 # 100MiB

      # Accumulate changes before storing improves performance with the
      # risk of losing the last writes if the JVM crashes.
      # The interval is by default set to 'off' to write each update immediately.
      # Enabling write behind by specifying a duration, e.g. 200ms, is especially
      # efficient when performing many writes to the same key, because it is only
      # the last value for each key that will be serialized and stored.
      # write-behind-interval = 200 ms
      write-behind-interval = off
    }
  }
}
# //#sharding-ext-config

akka.cluster.sharded-daemon-process {
  # Settings for the sharded dameon process internal usage of sharding are using the akka.cluste.sharding defaults.
  # Some of the settings can be overriden specifically for the sharded daemon process here. For example can the
  # `role` setting limit what nodes the daemon processes and the keep alive pingers will run on.
  # Some settings can not be changed (remember-entitites and related settings, passivation, number-of-shards),
  # overriding those settings will be ignored.
  sharding = ${akka.cluster.sharding}

  # Each entity is pinged at this interval from each node in the
  # cluster to trigger a start if it has stopped, for example during
  # rebalancing.
  # Note: How the set of actors is kept alive may change in the future meaning this setting may go away.
  keep-alive-interval = 10s
}

# Protobuf serializer for Cluster Sharding messages
akka.actor {
  serializers {
    akka-sharding = "Akka.Cluster.Sharding.Serialization.ClusterShardingMessageSerializer, Akka.Cluster.Sharding"
  }
  serialization-bindings {
    "Akka.Cluster.Sharding.IClusterShardingSerializable, Akka.Cluster.Sharding" = akka-sharding
  }
  serialization-identifiers {
    "Akka.Cluster.Sharding.Serialization.ClusterShardingMessageSerializer, Akka.Cluster.Sharding" = 13
  }
}
